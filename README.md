# RefDet - PhÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng dá»±a trÃªn áº£nh tham chiáº¿u

## ğŸ“‹ Giá»›i thiá»‡u

RefDet phÃ¡t hiá»‡n váº­t thá»ƒ nhá» trong video drone báº±ng cÃ¡ch so khá»›p áº£nh tham chiáº¿u (template) vá»›i áº£nh tÃ¬m kiáº¿m (search).

**Kiáº¿n trÃºc**: EfficientNet-B3 backbone + Transformer (4 layers, 8 heads) + Patch-based detection (4Ã—4 grid)

## âš™ï¸ CÃ i Ä‘áº·t

```bash
pip install -r requirements.txt
```

## ğŸš€ Training

### Training tá»« Ä‘áº§u

```bash
cd refdet
python train.py \
    --data_dir /path/to/dataset \
    --output_dir outputs_v2 \
    --batch_size 8 \
    --epochs 60 \
    --lr 1e-4 \
    --weight_decay 1e-4 \
    --augment_prob 0.4 \
    --num_heads 8 \
    --num_layers 4 \
    --dropout 0.1 \
    --workers 4
```

### Resume tá»« checkpoint

```bash
cd refdet
python train.py \
    --data_dir /path/to/dataset \
    --output_dir outputs_v2 \
    --checkpoint_path outputs_v2/checkpoint_epoch_2.pth \
    --batch_size 60 \
    --epochs 12 \
    --lr 1e-4 \
    --weight_decay 1e-4 \
    --augment_prob 0.4 \
    --num_heads 8 \
    --num_layers 4 \
    --dropout 0.1 \
    --workers 4
```

**LÆ°u Ã½**: Khi resume, cÃ¡c tham sá»‘ model (`num_heads`, `num_layers`, `dropout`) pháº£i khá»›p vá»›i checkpoint. CÃ¡c tham sá»‘ training (`lr`, `batch_size`, `augment_prob`) cÃ³ thá»ƒ thay Ä‘á»•i.

### Tham sá»‘ quan trá»ng

- `--augment_prob`: XÃ¡c suáº¥t augment (máº·c Ä‘á»‹nh 0.2 = 20%)
- `--checkpoint_path`: ÄÆ°á»ng dáº«n checkpoint Ä‘á»ƒ resume training
- `--batch_size`: Máº·c Ä‘á»‹nh 16, giáº£m xuá»‘ng 8 náº¿u GPU < 16GB
- `--epochs`: Sá»‘ epoch (máº·c Ä‘á»‹nh 80)

## ğŸ” Inference

```bash
cd refdet
python inference.py \
    --checkpoint_path outputs_v2/best_model_epoch_X_mIoU_X.XXXX.pth \
    --data_dir /path/to/dataset \
    --split public_test \
    --output_dir ./results \
    --confidence_threshold 0.5
```

### Tham sá»‘ inference

- `--checkpoint_path`: ÄÆ°á»ng dáº«n model checkpoint (báº¯t buá»™c)
- `--data_dir`: ThÆ° má»¥c dataset root (báº¯t buá»™c)
- `--split`: Dataset split (máº·c Ä‘á»‹nh: `public_test`)
- `--output_dir`: ThÆ° má»¥c output - sáº½ lÆ°u `submission.json` trong thÆ° má»¥c nÃ y (báº¯t buá»™c)
- `--confidence_threshold`: NgÆ°á»¡ng confidence (máº·c Ä‘á»‹nh: 0.5)

## ğŸ“ Cáº¥u trÃºc Dataset

```
dataset/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ templates/          # áº¢nh tham chiáº¿u
â”‚   â””â”€â”€ search/
â”‚       â”œâ”€â”€ images/         # Frame Ä‘Ã£ trÃ­ch
â”‚       â””â”€â”€ labels/         # NhÃ£n YOLO (class x_c y_c w h)
â”œâ”€â”€ val/
â”‚   â””â”€â”€ ... (tÆ°Æ¡ng tá»±)
â””â”€â”€ public_test/
    â”œâ”€â”€ templates/
    â””â”€â”€ search/
        â””â”€â”€ images/
```

## ğŸ’¾ Checkpoint

- `best_model_epoch_X_mIoU_X.XXXX.pth`: Model tá»‘t nháº¥t (theo mIoU)
- `last_model_epoch_X.pth`: Model epoch cuá»‘i
- `checkpoint_epoch_X.pth`: Checkpoint Ä‘áº§y Ä‘á»§ (model + optimizer + scaler) - lÆ°u má»—i 20 epochs

## ğŸ“Š Metrics

- **mIoU**: Mean IoU sau khi decode bbox
- **Patch Accuracy**: Tá»‰ lá»‡ patch Ä‘Æ°á»£c phÃ¢n loáº¡i Ä‘Ãºng
- **Loss**: Classification loss + Regression loss

## ğŸ§  YÃªu cáº§u VRAM

- **Inference**: ~1-2 GB
- **Training (batch=8)**: ~6-8 GB (FP16) / ~12-15 GB (FP32)
- **Training (batch=16)**: ~23-24 GB (FP32)

**Khuyáº¿n nghá»‹**: GPU â‰¥ 8GB, dÃ¹ng FP16 + batch_size=8

## ğŸ—‚ï¸ Xá»­ lÃ½ dá»¯ liá»‡u

### 1. TrÃ­ch xuáº¥t frame + template

```bash
cd data_process
python prepare_retrieval_dataset_flat.py \
    --source_dir ../train \
    --output_dir ../retrieval_dataset_flat
```

### 2. Fix label (má»—i file 1 bbox)

```bash
python fix_labels.py --data_dir ../retrieval_dataset_flat
```

### 3. Táº¡o dataset zoom (tÃ¹y chá»n)

```bash
python create_zoomed_dataset.py \
    --source_dir ../retrieval_dataset_flat \
    --output_dir ../retrieval_dataset_flat_zoomed \
    --area_ratio1 0.15 \
    --area_ratio2 0.35 \
    --area_ratio3 0.55 \
    --area_ratio4 0.75
```

## ğŸ“ Ghi chÃº

- Model sá»­ dá»¥ng **Mixed Precision (AMP)** tá»± Ä‘á»™ng Ä‘á»ƒ tiáº¿t kiá»‡m VRAM
- **Augment probability**: 20% Ä‘á»ƒ giá»¯ phÃ¢n phá»‘i dá»¯ liá»‡u gá»‘c
- **Output format**: submission.json theo format yÃªu cáº§u vá»›i `video_id`, `detections`, `bboxes` (frame, x1, y1, x2, y2)

## ğŸ—ï¸ Kiáº¿n trÃºc Model

### SÆ¡ Ä‘á»“ tá»•ng quan

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Enhanced Siamese Detector V2                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT: Template (640Ã—640Ã—3)          INPUT: Search (640Ã—640Ã—3)
         â”‚                                    â”‚
         â–¼                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EfficientNet-B3   â”‚            â”‚  EfficientNet-B3   â”‚
â”‚   (Shared)         â”‚            â”‚   (Shared)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                    â”‚
    (B,512,H,W)                          (B,512,H,W)
         â”‚                                    â”‚
         â–¼                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AdaptiveAvgPool2d  â”‚            â”‚   PatchEmbedding   â”‚
â”‚      (4Ã—4)         â”‚            â”‚    (4Ã—4 grid)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                    â”‚
    (B,512,4,4)                          (B,16,512)
         â”‚                                    â”‚
         â–¼                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Flatten + Permuteâ”‚            â”‚  Self-Attention     â”‚
â”‚   + LayerNorm      â”‚            â”‚  (4 layers)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                    â”‚
    (B,16,512)                           (B,16,512)
         â”‚                                    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Cross-Attention   â”‚
              â”‚  Q: Search patches â”‚
              â”‚  K,V: Ref tokens    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                   (B,16,512)
                        â”‚
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Final LayerNorm  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Spatial Refine    â”‚
              â”‚  Linear+Norm+ReLU  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                   (B,16,512)
                        â”‚
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Reshape to 4Ã—4   â”‚
              â”‚  (B,512,4,4)       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Conv Refinement   â”‚
              â”‚  2Ã— Conv2d+BN+ReLUâ”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                   (B,256,4,4)
                        â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                             â”‚
         â–¼                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cls Head       â”‚          â”‚  Reg Head        â”‚
â”‚  Flatten        â”‚          â”‚  Flatten         â”‚
â”‚  MLP(256â†’128â†’64)â”‚          â”‚  MLP(256â†’128â†’64) â”‚
â”‚  Sigmoid        â”‚          â”‚  Linear(64â†’64)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                             â”‚
    (B,16,1)                      (B,16,4)
         â”‚                             â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Output           â”‚
              â”‚   - cls_probs       â”‚
              â”‚   - bbox_deltas     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Chi tiáº¿t cÃ¡c thÃ nh pháº§n

#### 1. Backbone (EfficientNet-B3)
- **Input**: 640Ã—640Ã—3
- **Output**: (B, 512, H, W) sau projection head
- **Shared**: Template vÃ  Search dÃ¹ng chung backbone

#### 2. Template Processing
```
Template (640Ã—640Ã—3)
  â†’ EfficientNet-B3 â†’ (B, 512, H, W)
  â†’ AdaptiveAvgPool2d(4Ã—4) â†’ (B, 512, 4, 4)
  â†’ Flatten(2) + Permute(0,2,1) â†’ (B, 16, 512)
  â†’ LayerNorm â†’ (B, 16, 512) [Reference Tokens]
```

#### 3. Search Processing
```
Search (640Ã—640Ã—3)
  â†’ EfficientNet-B3 â†’ (B, 512, H, W)
  â†’ PatchEmbedding (4Ã—4 grid) â†’ (B, 16, 512)
  â†’ Self-Attention (4 layers) â†’ (B, 16, 512)
```

#### 4. Cross-Attention
- **Query**: Search patches (B, 16, 512)
- **Key/Value**: Reference tokens (B, 16, 512)
- **Output**: Attended patches (B, 16, 512)

#### 5. Detection Heads
```
Attended patches (B, 16, 512)
  â†’ Spatial Refine (Linear+Norm+ReLU) â†’ (B, 16, 512)
  â†’ Reshape to 4Ã—4 â†’ (B, 512, 4, 4)
  â†’ Conv Refinement (2Ã— Conv2d) â†’ (B, 256, 4, 4)
  â†’ Classification Head: Flatten â†’ MLP â†’ Sigmoid â†’ (B, 16, 1)
  â†’ Regression Head: Flatten â†’ MLP â†’ (B, 16, 4)
```

### Tham sá»‘ Model

- **Backbone**: EfficientNet-B3 (pretrained ImageNet)
- **Embedding dim**: 512
- **Patch grid**: 4Ã—4 = 16 patches
- **Transformer layers**: 4 (self-attention)
- **Attention heads**: 8
- **Dropout**: 0.1
- **Total params**: ~31.71M
