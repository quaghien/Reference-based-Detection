# K·ªπ Thu·∫≠t √Åp D·ª•ng trong Reference-based Detection

T√†i li·ªáu n√†y m√¥ t·∫£ chi ti·∫øt c√°c k·ªπ thu·∫≠t ƒë√£ ƒë∆∞·ª£c √°p d·ª•ng trong h·ªá th·ªëng Reference-based Detection cho drone surveillance, bao g·ªìm ki·∫øn tr√∫c model, loss functions, v√† c√°c k·ªπ thu·∫≠t training.

---

## üìê 1. Ki·∫øn Tr√∫c Model

### 1.1 Siamese Network Architecture

**M√¥ t·∫£:** S·ª≠ d·ª•ng ki·∫øn tr√∫c Siamese v·ªõi shared backbone ƒë·ªÉ extract features t·ª´ c·∫£ template v√† search images.

**√ù t∆∞·ªüng:**
- Template (reference image) v√† Search (query frame) d√πng chung m·ªôt backbone
- ƒê·∫£m b·∫£o feature space consistency gi·ªØa reference v√† query
- Hi·ªáu qu·∫£ v·ªÅ tham s·ªë v√† training stability

**Paper tham kh·∫£o:**
- Siamese Neural Networks for One-shot Image Recognition (2015)
- Fully-Convolutional Siamese Networks for Object Tracking (2016)

**Implementation:**
```python
# Shared EfficientNet-B3 backbone
template_feat = backbone(template)  # (B, 512, H, W)
search_feat = backbone(search)      # (B, 512, H, W)
```

---

### 1.2 EfficientNet-B3 Backbone

**M√¥ t·∫£:** S·ª≠ d·ª•ng EfficientNet-B3 l√†m feature extractor v·ªõi pretrained ImageNet weights.

**∆Øu ƒëi·ªÉm:**
- Compound scaling (depth, width, resolution) (2019)
- Hi·ªáu qu·∫£ v·ªÅ tham s·ªë v√† FLOPs
- T·ªët cho small object detection nh·ªù multi-scale features

**Paper tham kh·∫£o:**
- EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (2019)

**Configuration:**
- Input: 640√ó640√ó3
- Output: (B, 512, H, W) sau projection head
- Pretrained: ImageNet weights

---

### 1.3 Patch-based Detection

**M√¥ t·∫£:** Chia search image th√†nh 4√ó4 grid (16 patches) ƒë·ªÉ detect object ·ªü t·ª´ng patch.

**√ù t∆∞·ªüng:**
- Thay v√¨ predict to√†n b·ªô image, predict t·ª´ng patch
- M·ªói patch c√≥ classification score v√† bbox regression
- Ph√π h·ª£p v·ªõi small object detection trong drone surveillance

**∆Øu ƒëi·ªÉm:**
- TƒÉng resolution cho small objects
- Localize ch√≠nh x√°c h∆°n
- Gi·∫£m false positives

**Paper tham kh·∫£o:**
- You Only Look Once: Unified, Real-Time Object Detection (2016)
- FCOS: Fully Convolutional One-Stage Object Detection (2019)

**Implementation:**
```python
# Split search features into 4√ó4 grid
search_patches = patch_embed(search_feat)  # (B, 16, 512)
```

---

### 1.4 Transformer Architecture

#### 1.4.1 Self-Attention Layers

**M√¥ t·∫£:** Self-attention gi·ªØa c√°c search patches ƒë·ªÉ capture spatial relationships.

**√ù t∆∞·ªüng:**
- M·ªói patch attend ƒë·∫øn t·∫•t c·∫£ patches kh√°c
- H·ªçc ƒë∆∞·ª£c context v√† spatial dependencies
- Quan tr·ªçng cho vi·ªác ph√¢n bi·ªát object vs background

**Paper tham kh·∫£o:**
- Attention Is All You Need (2017)
- Vision Transformer (ViT) (2020)

**Configuration:**
- Number of layers: 4
- Number of heads: 8
- Embedding dim: 512
- Dropout: 0.1

#### 1.4.2 Cross-Attention

**M√¥ t·∫£:** Cross-attention gi·ªØa search patches (Query) v√† reference tokens (Key/Value).

**√ù t∆∞·ªüng:**
- Search patches query information t·ª´ reference features
- Match template v·ªõi search patches
- TƒÉng accuracy cho reference-based detection

**Paper tham kh·∫£o:**
- Attention Is All You Need (2017)
- DETR: End-to-End Object Detection with Transformers (2020)

**Implementation:**
```python
# Cross-attention: Search patches attend to reference
attended_patches = cross_attn(search_patches, ref_tokens)
```

---

### 1.5 Detection Heads

**M√¥ t·∫£:** Hai heads ri√™ng bi·ªát cho classification v√† regression.

**Architecture:**
- **Classification Head:** MLP ‚Üí Sigmoid ‚Üí (B, 16, 1)
- **Regression Head:** MLP ‚Üí (B, 16, 4) - bbox deltas

**Paper tham kh·∫£o:**
- Faster R-CNN: Towards Real-Time Object Detection (2015)
- FCOS: Fully Convolutional One-Stage Object Detection (2019)

---

## üéØ 2. Loss Functions

### 2.1 Focal Loss

**M√¥ t·∫£:** Focal Loss ƒë·ªÉ x·ª≠ l√Ω class imbalance (15 negative patches : 1 positive patch).

**C√¥ng th·ª©c:**
```
FL(p_t) = -Œ±_t * (1 - p_t)^Œ≥ * log(p_t)
```

**Tham s·ªë:**
- `Œ± = 0.25`: Weighting factor cho positive class
- `Œ≥ = 2.0`: Focusing parameter (focus on hard examples)

**∆Øu ƒëi·ªÉm:**
- Gi·∫£m weight c·ªßa easy negatives
- Focus v√†o hard examples
- C·∫£i thi·ªán recall cho small objects

**Paper tham kh·∫£o:**
- Focal Loss for Dense Object Detection (2017)

**Implementation:**
```python
focal_loss(pred_probs, targets, alpha=0.25, gamma=2.0, smooth=0.05)
```

---

### 2.2 Label Smoothing

**M√¥ t·∫£:** Label smoothing ƒë·ªÉ tr√°nh overconfidence v√† tƒÉng robustness.

**C√¥ng th·ª©c:**
```
target_smooth = target * (1 - smooth) + smooth / 2
```

**K·∫øt qu·∫£:**
- Hard label = 1.0 ‚Üí 0.975 (v·ªõi smooth=0.05)
- Hard label = 0.0 ‚Üí 0.025
- Soft label (IoU) gi·ªØ ƒë∆∞·ª£c ch√™nh l·ªách

**∆Øu ƒëi·ªÉm:**
- Tr√°nh overconfident predictions
- Robust v·ªõi noise trong video
- T·ªët cho small object detection

**Paper tham kh·∫£o:**
- Rethinking the Inception Architecture for Computer Vision (2016)
- When Does Label Smoothing Help? (2019)

**Integration:**
- √Åp d·ª•ng trong Focal Loss ƒë·ªÉ smooth targets tr∆∞·ªõc khi compute cross-entropy

---

### 2.3 IoU-based Soft Labels

**M√¥ t·∫£:** S·ª≠ d·ª•ng IoU values l√†m soft targets thay v√¨ hard labels (0/1).

**√ù t∆∞·ªüng:**
- Patch c√≥ IoU cao v·ªõi object ‚Üí target cao
- Patch c√≥ IoU th·∫•p ‚Üí target th·∫•p
- Ph√π h·ª£p v·ªõi patch-based detection

**C√¥ng th·ª©c:**
```python
# Compute IoU between patch bbox and object bbox
iou = compute_patch_object_iou(patch_bbox, obj_bbox)
heatmap[patch_idx] = iou  # Use IoU as soft target
```

**∆Øu ƒëi·ªÉm:**
- Fine-grained supervision
- T·ªët cho small objects (IoU = 0.15 v·∫´n c√≥ signal)
- T·ªët cho objects spanning multiple patches

**Paper tham kh·∫£o:**
- IoU-aware Single-stage Object Detector for Accurate Localization (2019)
- Soft Labels for Object Detection (2020)

**Implementation:**
```python
# In make_patch_heatmaps()
for patch in patches:
    iou = compute_patch_object_iou(patch_bbox, obj_bbox)
    heatmap[patch_idx] = iou  # Soft target
```

---

### 2.4 Object Size Normalization

**M√¥ t·∫£:** Weight loss theo s·ªë l∆∞·ª£ng positive patches ƒë·ªÉ balance gi·ªØa small v√† large objects.

**C√¥ng th·ª©c:**
```python
num_pos_patches = pos_mask.sum(dim=1)  # (B,)
size_weights = 1.0 / sqrt(num_pos_patches + 1e-6)
weighted_loss = (size_weights * loss_per_sample).mean()
```

**K·∫øt qu·∫£:**
- Small object (1 patch) ‚Üí weight = 1.0
- Large object (4 patches) ‚Üí weight = 0.5
- Medium object (2 patches) ‚Üí weight = 0.707

**∆Øu ƒëi·ªÉm:**
- Tr√°nh large objects dominate training
- TƒÉng focus v√†o small objects
- Balance detection performance

**Paper tham kh·∫£o:**
- Focal Loss for Dense Object Detection (2017) - similar idea for class imbalance
- Learning to Balance: Importance Sampling for Object Detection (2019)

**Implementation:**
```python
# Per-sample weighting
cls_loss_per_sample = focal_loss(..., reduction='none')  # (B,)
weighted_cls_loss = (size_weights * cls_loss_per_sample).mean()
```

---

### 2.5 Smooth L1 Loss (Regression)

**M√¥ t·∫£:** Smooth L1 loss cho bbox regression, ch·ªâ t√≠nh tr√™n positive patches.

**C√¥ng th·ª©c:**
```
smooth_l1(x) = {
    0.5 * x^2  if |x| < 1
    |x| - 0.5  otherwise
}
```

**∆Øu ƒëi·ªÉm:**
- Robust v·ªõi outliers
- Smooth gradient
- Ch·ªâ t√≠nh tr√™n positive patches (efficient)

**Paper tham kh·∫£o:**
- Fast R-CNN (2015)
- Faster R-CNN: Towards Real-Time Object Detection (2015)

**Weight:** `reg_weight = 2.0` (classification loss ƒë∆∞·ª£c weight b·ªüi size_weights)

---

## üîß 3. Training Techniques

### 3.1 Hard Mining

**M√¥ t·∫£:** Oversample hard samples (small objects, near boundaries, elongated) ƒë·ªÉ tƒÉng focus v√†o difficult cases.

**Hard Criteria:**
1. **Small objects:** `area < 0.01` (objects < 64px in 640√ó640)
2. **Near boundaries:** Object center g·∫ßn patch boundaries
3. **Elongated objects:** `aspect_ratio > 3.0`

**Implementation:**
- Oversample 33% hard samples trong dataset
- `__len__()` returns `len(samples) + len(hard_samples) // 3`
- `__getitem__()` maps indices to hard samples khi c·∫ßn

**∆Øu ƒëi·ªÉm:**
- TƒÉng focus v√†o difficult cases
- C·∫£i thi·ªán recall cho small objects
- Faster convergence

**Paper tham kh·∫£o:**
- Training Region-based Object Detectors with Online Hard Example Mining (2016)
- Focal Loss for Dense Object Detection (2017) - hard example mining concept

---

### 3.2 Data Augmentation

#### 3.2.1 Geometric Augmentation

**M√¥ t·∫£:** Geometric transformations (rotation, flip, affine) √°p d·ª•ng ƒë·ªìng b·ªô cho template v√† search.

**Transformations:**
- **Rotation:** ¬±5¬∞ (reduced from ¬±10¬∞)
- **Horizontal flip:** 50% probability
- **Vertical flip:** 30% probability
- **Affine:** Translation (¬±5%), Scale (0.95-1.05), Shear (¬±3¬∞)

**Bbox Transformation:**
- Transform bbox coordinates theo ƒë√∫ng geometric augmentations
- S·ª≠ d·ª•ng `transform_bbox()` ƒë·ªÉ convert 4 corners ‚Üí center-based format

**∆Øu ƒëi·ªÉm:**
- TƒÉng data diversity
- Robust v·ªõi camera motion, rotation
- Maintain template-search alignment

**Paper tham kh·∫£o:**
- Data Augmentation for Object Detection (2017)
- Learning Data Augmentation Strategies for Object Detection (2019)

#### 3.2.2 Color Augmentation

**M√¥ t·∫£:** Color jitter (brightness, contrast, saturation) √°p d·ª•ng ƒë·ªìng b·ªô.

**Parameters:**
- **Brightness:** ¬±30% (reduced from ¬±40%)
- **Contast:** ¬±20% (reduced from ¬±30%)
- **Saturation:** ¬±20% (reduced from ¬±30%)

**L∆∞u √Ω:** Color augmentation √°p d·ª•ng c√πng cho template v√† search ƒë·ªÉ maintain feature matching.

**Paper tham kh·∫£o:**
- ImageNet Classification with Deep Convolutional Neural Networks (2012)
- AutoAugment: Learning Augmentation Strategies from Data (2018)

---

### 3.3 Gradient Clipping

**M√¥ t·∫£:** Clip gradients ƒë·ªÉ prevent exploding gradients v√† NaN losses.

**Implementation:**
```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

**∆Øu ƒëi·ªÉm:**
- Stable training
- Prevent NaN losses
- ƒê·∫∑c bi·ªát quan tr·ªçng v·ªõi FP32 training

**Paper tham kh·∫£o:**
- On the difficulty of training Recurrent Neural Networks (2013)
- Deep Residual Learning for Image Recognition (2016)

---

### 3.4 Learning Rate Scheduling

**M√¥ t·∫£:** H·ªó tr·ª£ 3 lo·∫°i LR schedule: constant, cosine, linear.

**Options:**
1. **Constant:** Fixed learning rate
2. **Cosine:** Cosine annealing v·ªõi `min_lr`
3. **Linear:** Linear decay v·ªõi `min_lr`

**Configuration:**
- Default LR: `1e-4`
- Min LR: `lr * 0.01` (ho·∫∑c custom `--min_lr`)
- Cosine: `T_max = epochs`, `eta_min = min_lr`

**Paper tham kh·∫£o:**
- SGDR: Stochastic Gradient Descent with Warm Restarts (2016)
- Super-Convergence: Very Fast Training of Neural Networks (2017)

---

## üìä 4. Data Processing

### 4.1 Bounding Box Encoding/Decoding

#### 4.1.1 Encoding (Ground Truth ‚Üí Model Format)

**M√¥ t·∫£:** Convert normalized bbox (x_c, y_c, w, h) th√†nh patch-relative deltas.

**Process:**
1. Identify positive patches (IoU > 0.3 v·ªõi object)
2. Compute deltas t·ª´ patch center ƒë·∫øn object center
3. Normalize deltas by patch size

**IoU-based Assignment:**
```python
# Patch is positive if IoU > threshold (default 0.3)
for patch in patches:
    iou = compute_patch_object_iou(patch_bbox, obj_bbox)
    if iou > 0.3:
        patch_pos_mask[patch_idx] = 1
        patch_deltas[patch_idx] = compute_deltas(...)
```

**Paper tham kh·∫£o:**
- You Only Look Once: Unified, Real-Time Object Detection (2016)
- FCOS: Fully Convolutional One-Stage Object Detection (2019)

#### 4.1.2 Decoding (Model Output ‚Üí Bbox)

**M√¥ t·∫£:** Convert patch deltas v·ªÅ normalized bbox coordinates.

**Process:**
1. Get best patch (highest classification score)
2. Decode bbox t·ª´ patch center + deltas
3. Clamp to [0, 1] range

**Implementation:**
```python
best_patch_idx = cls_probs.argmax(dim=1)
bbox = decode_patch_bbox(patch_idx, deltas, patch_grid_info)
```

---

### 4.2 Bbox Transformation for Augmentation

**M√¥ t·∫£:** Transform bbox coordinates khi apply geometric augmentation.

**Method:**
1. Convert center-based (x_c, y_c, w, h) ‚Üí 4 corners
2. Apply transformations (rotation, flip, affine)
3. Convert back to center-based format

**Transformations:**
- Rotation: around image center
- Flip: mirror coordinates
- Affine: translate ‚Üí rotate ‚Üí scale ‚Üí shear

**Paper tham kh·∫£o:**
- Data Augmentation for Object Detection (2017)
- Learning Data Augmentation Strategies for Object Detection (2019)

---

## üéì 5. T·ªïng K·∫øt

### 5.1 K·ªπ Thu·∫≠t Ch√≠nh

| K·ªπ Thu·∫≠t | M·ª•c ƒê√≠ch | Paper |
|----------|----------|-------|
| Siamese Network | Feature consistency | Siamese Networks (2015) |
| EfficientNet-B3 | Efficient backbone | EfficientNet (2019) |
| Patch-based Detection | Small object detection | YOLO (2016), FCOS (2019) |
| Transformer (Self/Cross-Attn) | Spatial relationships | Attention Is All You Need (2017) |
| Focal Loss | Class imbalance | Focal Loss (2017) |
| Label Smoothing | Robustness | Inception v3 (2016) |
| IoU Soft Labels | Fine-grained supervision | IoU-aware Detection (2019) |
| Size Normalization | Balance small/large objects | Focal Loss (2017) |
| Hard Mining | Focus on difficult cases | OHEM (2016) |
| Gradient Clipping | Training stability | ResNet (2016) |

### 5.2 Expected Improvements

- **Small Objects:** Recall tƒÉng 15-25% (soft labels + size weighting)
- **Hard Negatives:** Precision tƒÉng 10-15% (focal loss)
- **Convergence:** Nhanh h∆°n 2-3x (hard mining)
- **Robustness:** T·ªët h∆°n v·ªõi video noise (label smoothing)

---

## üìö 6. References

### Papers

1. **Siamese Networks:**
   - Siamese Neural Networks for One-shot Image Recognition (2015)
   - Fully-Convolutional Siamese Networks for Object Tracking (2016)

2. **EfficientNet:**
   - EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (2019)

3. **Object Detection:**
   - You Only Look Once: Unified, Real-Time Object Detection (2016)
   - Faster R-CNN: Towards Real-Time Object Detection (2015)
   - FCOS: Fully Convolutional One-Stage Object Detection (2019)
   - IoU-aware Single-stage Object Detector for Accurate Localization (2019)

4. **Transformers:**
   - Attention Is All You Need (2017)
   - Vision Transformer (ViT) (2020)
   - DETR: End-to-End Object Detection with Transformers (2020)

5. **Loss Functions:**
   - Focal Loss for Dense Object Detection (2017)
   - Rethinking the Inception Architecture for Computer Vision (2016)
   - When Does Label Smoothing Help? (2019)

6. **Training Techniques:**
   - Training Region-based Object Detectors with Online Hard Example Mining (2016)
   - Deep Residual Learning for Image Recognition (2016)
   - SGDR: Stochastic Gradient Descent with Warm Restarts (2016)

7. **Data Augmentation:**
   - ImageNet Classification with Deep Convolutional Neural Networks (2012)
   - Data Augmentation for Object Detection (2017)
   - Learning Data Augmentation Strategies for Object Detection (2019)
   - AutoAugment: Learning Augmentation Strategies from Data (2018)

---

**Last Updated:** 2024

